{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gyarbete.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qSZR_4fL4M1"
      },
      "source": [
        "This is my model for my final work in upper secondary school"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0t9ya4eL02s",
        "outputId": "e161fafb-2574-4e38-eb68-447705717632"
      },
      "source": [
        "# The first installed libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import Files and Directories for training\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "path = \"/content/gdrive/MyDrive/Colab_Notebooks/ML/\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcUmYa43M7CY"
      },
      "source": [
        "# check that it is mounted properly\n",
        "!ls gdrive/MyDrive/Colab_Notebooks/ML/"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cyg4jxvpPrGV"
      },
      "source": [
        "This is part of my GitHub repo, https://www.github.com/Irreq/gyarbete\n",
        "\n",
        "Used for synthetic data generation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcsDrBbVRHiU",
        "outputId": "f8921a2b-b69d-4548-da06-8d30e3d001dd"
      },
      "source": [
        "from sklearn.neighbors.kde import KernelDensity\n",
        "\n",
        "from scipy.signal import argrelextrema\n",
        "\n",
        "\n",
        "\n",
        "class KernelGenerator(object):\n",
        "\n",
        "    def __init__(self, size=200, debug=False):\n",
        "\n",
        "        self.data = None\n",
        "\n",
        "        self.size = size\n",
        "\n",
        "        self.debug = debug\n",
        "\n",
        "        self.bias = {\n",
        "\n",
        "            \"normal_distribution\" : [[0.5, 0.2],], # median, standard deviation\n",
        "\n",
        "        }\n",
        "\n",
        "    def kernel_normal_dist(self, mean, std, size=None, window=[0, 1]):\n",
        "\n",
        "        \"\"\" Returns normal distribution \"\"\"\n",
        "\n",
        "        if size == None:\n",
        "            size = self.size\n",
        "\n",
        "        s = np.random.normal(mean, std, size)\n",
        "\n",
        "        min_val, max_val = window[0], window[1]\n",
        "\n",
        "        return [i for i in s if min_val <= i <= max_val]\n",
        "\n",
        "    def kernel_density(self):\n",
        "\n",
        "        pre_data = self.data\n",
        "\n",
        "        start = np.array(pre_data)\n",
        "\n",
        "        start_len = len(start)\n",
        "\n",
        "        resolution = np.linspace(0, 1, num=10).tolist()\n",
        "\n",
        "        pre_data = np.histogram(pre_data, bins=resolution)[0]\n",
        "\n",
        "        pre_data = pre_data / max(pre_data)\n",
        "\n",
        "        pre_data = np.array([int(i*100) for i in pre_data.tolist()])\n",
        "\n",
        "        initial_length = int(len(pre_data) * 2) # 2 is an arbitary good number to use\n",
        "\n",
        "        a = pre_data.reshape(-1, 1)\n",
        "\n",
        "        kde = KernelDensity(kernel='gaussian', bandwidth=2).fit(a)\n",
        "        s = np.linspace(0, initial_length)\n",
        "        e = kde.score_samples(s.reshape(-1, 1))\n",
        "\n",
        "        lower_boundaries = argrelextrema(e, np.less)[0]\n",
        "\n",
        "        minima = s[lower_boundaries]\n",
        "\n",
        "        demodulated_index = [int((i/initial_length)*start_len) for i in minima]\n",
        "\n",
        "        return start[np.array(demodulated_index)]\n",
        "\n",
        "\n",
        "    def kernel_generator(self, localkernel):\n",
        "\n",
        "        normal_distributions = []\n",
        "\n",
        "        for k in range(len(localkernel)):\n",
        "\n",
        "            distribution = self.kernel_normal_dist(localkernel[k][0], localkernel[k][1], self.size)\n",
        "\n",
        "            normal_distributions.extend(distribution)\n",
        "\n",
        "\n",
        "        normal_distributions.sort()\n",
        "\n",
        "        self.data = normal_distributions[::int(round(len(normal_distributions)/self.size))]\n",
        "\n",
        "        if len(self.data) > self.size:\n",
        "\n",
        "            for i in range(len(self.data)-self.size):\n",
        "\n",
        "                del self.data[np.random.randint(len(self.data))]\n",
        "\n",
        "\n",
        "        elif len(self.data) < self.size:\n",
        "\n",
        "            local_average = np.mean(np.array(self.data))\n",
        "\n",
        "            values = self.kernel_density()[0]\n",
        "\n",
        "            values = self.kernel_normal_dist(values, 0.1, size=self.size-len(self.data))\n",
        "\n",
        "            values = [(i+local_average)/2 for i in values]\n",
        "\n",
        "            self.data.extend(values)\n",
        "\n",
        "            self.data.sort()\n",
        "\n",
        "\n",
        "\n",
        "        if self.debug:\n",
        "\n",
        "            import matplotlib.pyplot as plt\n",
        "\n",
        "            print('elements : {}\\nmean : {}\\nmax : {}\\nmin : {}'.format(len(self.data),np.mean(np.array(self.data)), max(self.data), min(self.data)))\n",
        "\n",
        "            plt.hist(np.array(self.data), bins=np.linspace(0,1, num=100).tolist())\n",
        "            plt.show()\n",
        "\n",
        "        return self.data\n",
        "\n",
        "    def kernel_error_catcher(self, kernel_seed):\n",
        "\n",
        "        ErrorCount = 0\n",
        "\n",
        "        while True:\n",
        "\n",
        "            try:\n",
        "                return self.kernel_generator(kernel_seed)\n",
        "\n",
        "                break\n",
        "\n",
        "            except Exception as e:\n",
        "\n",
        "                ErrorCount += 1\n",
        "\n",
        "                if ErrorCount > 100:\n",
        "                    print('Error overflow')\n",
        "                    print(e)\n",
        "                    print(\"error found in kernel density estimation last row\")\n",
        "                    break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def random_kernels(self, n_kernels):\n",
        "\n",
        "        self.bias['normal_distribution'] = self.kernel_error_catcher(self.bias['normal_distribution'])\n",
        "\n",
        "        def dub(n):\n",
        "\n",
        "            return [0.2, 0.3], [0.8, 0.7]\n",
        "\n",
        "        for i in range(n_kernels):\n",
        "\n",
        "            x = np.random.choice(self.bias['normal_distribution'])\n",
        "\n",
        "            print(\"normal_distribution\")\n",
        "\n",
        "            print(x)\n",
        "\n",
        "            self.bias[i] = [dub(x)]\n",
        "\n",
        "        return\n",
        "\n",
        "    def start(self):\n",
        "\n",
        "        for id in self.bias:\n",
        "\n",
        "            if type(self.bias[id][0]).__name__ == list:\n",
        "                continue\n",
        "\n",
        "            self.bias[id] = self.kernel_error_catcher(self.bias[id])\n",
        "\n",
        "    def addbias(self, *kernel):\n",
        "\n",
        "        \"\"\"\n",
        "        Adds distributions from a dictionary\n",
        "\n",
        "\n",
        "        kernel = eg, {'tag':[[0.4]]}\n",
        "\n",
        "        returns = dict() # distribution\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        if len(kernel) == 0:\n",
        "            return\n",
        "\n",
        "        else:\n",
        "            kernel = kernel[0]\n",
        "\n",
        "        for id in kernel.keys():\n",
        "            self.bias[id] = self.kernel_error_catcher(kernel[id])\n",
        "\n",
        "        return {id:self.bias[id] for id in kernel.keys()}\n",
        "\n",
        "    def getbias(self):\n",
        "\n",
        "        return self.bias\n",
        "\n",
        "    def setwindow(self, lower, upper, kernel_id):\n",
        "\n",
        "        if type(kernel_id) == str:\n",
        "\n",
        "            try:\n",
        "\n",
        "                data = self.bias[kernel_id]\n",
        "\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                print(\"Window {} < x < {} could not be set, due to:\".format(lower, upper))\n",
        "                return\n",
        "\n",
        "        elif type(kernel_id) == list:\n",
        "\n",
        "            data = kernel_id\n",
        "\n",
        "        else:\n",
        "            data = kernel_id\n",
        "\n",
        "        data = [i*(upper-lower)+lower for i in data]\n",
        "\n",
        "        if type(kernel_id) == str:\n",
        "\n",
        "            self.bias[kernel_id] = data\n",
        "\n",
        "        return data\n",
        "\n",
        "    def fastgen(self, distribution):\n",
        "\n",
        "        resolution = self.size\n",
        "\n",
        "        window = len(distribution) ** -1\n",
        "\n",
        "        nd = self.getbias()[\"normal_distribution\"]\n",
        "\n",
        "        window_distribution = self.setwindow(0,window,nd)\n",
        "\n",
        "        final_distribution = []\n",
        "\n",
        "        for i, value in enumerate(distribution):\n",
        "\n",
        "            data = [i*window+np.random.choice(window_distribution) for _ in range(int(value*resolution))]\n",
        "\n",
        "            final_distribution.extend(data)\n",
        "\n",
        "        return np.array(final_distribution)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.kde module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDBk4iVgQyfM"
      },
      "source": [
        "# =============== Start Borrowed Code ======================\n",
        "#\n",
        "# Author : Ian Cotter-Llewellyn\n",
        "# Source : https://github.com/ian-llewellyn/manchester-coding\n",
        "#\n",
        "# Code is untouched and just placed in\n",
        "# this file for ease of readability\n",
        "\n",
        "class Manchester(object):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Manchester(differential=True).encode(data)\n",
        "\n",
        "    # -*- coding: utf-8 -*-\n",
        "\n",
        "    G. E. Thomas: 0 = 01, 1 = 10\n",
        "    ISO 802.4: 0 = 10, 1 = 01\n",
        "\n",
        "    \"\"\"\n",
        "    _bit_symbol_map = {\n",
        "        # bit: symbol\n",
        "        '0': '01',\n",
        "        '1': '10',\n",
        "        'invert': {\n",
        "            # bit: symbol\n",
        "            '0': '10',\n",
        "            '1': '01'},\n",
        "        'differential': {\n",
        "            # (init_level, bit): symbol\n",
        "            ('1', '0'): '01',\n",
        "            ('0', '0'): '10',\n",
        "            ('0', '1'): '11',\n",
        "            ('1', '1'): '00'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    def __init__(self, differential=False, invert=False):\n",
        "        self._invert = invert\n",
        "        self._differential = differential\n",
        "        self._init_level = '0'\n",
        "\n",
        "    def invert(self):\n",
        "        self._invert = not self._invert\n",
        "\n",
        "    def differential(self):\n",
        "        self._differential = not self._differential\n",
        "\n",
        "    def decode(self, symbols):\n",
        "        bits = ''\n",
        "        while len(symbols):\n",
        "            symbol = symbols[0:2]\n",
        "            symbols = symbols[2:]\n",
        "\n",
        "            if self._differential:\n",
        "                for ib, s in self._bit_symbol_map['differential'].items():\n",
        "                    if symbol == s:\n",
        "                        bits += ib[1]\n",
        "                continue\n",
        "\n",
        "            if self._invert:\n",
        "                for b, s in self._bit_symbol_map['invert'].items():\n",
        "                    if symbol == s:\n",
        "                        bits += b\n",
        "                continue\n",
        "\n",
        "            for b, s in self._bit_symbol_map.items():\n",
        "                if symbol == s:\n",
        "                    bits += b\n",
        "\n",
        "        return bits\n",
        "\n",
        "    def encode(self, bits, init_level=None):\n",
        "        if init_level:\n",
        "            self._init_level = init_level\n",
        "\n",
        "        symbols = ''\n",
        "        for bit in bits:\n",
        "            # Differential Manchester Coding\n",
        "            if self._differential:\n",
        "                symbols += self._bit_symbol_map['differential'][(self._init_level, bit)]\n",
        "                self._init_level = symbols[-1]\n",
        "                continue\n",
        "\n",
        "            # IEEE 802.4 (Inverted Manchester Coding)\n",
        "            if self._invert:\n",
        "                symbols += self._bit_symbol_map['invert'][bit]\n",
        "                continue\n",
        "\n",
        "            # Manchester Coding\n",
        "            symbols += self._bit_symbol_map[bit]\n",
        "\n",
        "        return symbols\n",
        "\n",
        "# =============== End Borrowed Code ======================\n",
        "\n",
        "def str_to_bin(data_string, encoding=False):\n",
        "\n",
        "    \"\"\"\n",
        "    String to Binary String Converter Function\n",
        "\n",
        "     data_string : str()\n",
        "        encoding : Boolean\n",
        "\n",
        "         returns : str()\n",
        "    \"\"\"\n",
        "\n",
        "    binary = \"\".join(f\"{ord(i):08b}\" for i in data_string) # String to binary conversion\n",
        "\n",
        "    if encoding:\n",
        "\n",
        "        binary = Manchester(differential=True).encode(binary) # returns Manchester encoded data\n",
        "\n",
        "    return binary\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RBZ4cN-Qt90"
      },
      "source": [
        "# =============== Start Modulation =======================\n",
        "\n",
        "class Modulation(object):\n",
        "\n",
        "    def __init__(self, frequency=1e3, samplingrate=44.1e3,\n",
        "                        bitrate=10, amplitude=0.5, encoding=True):\n",
        "\n",
        "        \"\"\"\n",
        "            frequency : the frequency of the signal\n",
        "         samplingrate : the samplingrate, prefferably 44.1kHz\n",
        "              bitrate : float or integer\n",
        "            amplitude : prefferably <= 2.0\n",
        "             encoding : Boolean\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        assert float(frequency) > 0\n",
        "        self.frequency = frequency\n",
        "\n",
        "        assert float(samplingrate) > 2 * frequency, \"Insufficient Nyquist rate\"\n",
        "        self.samplingrate = samplingrate\n",
        "\n",
        "        assert float(bitrate) > 0\n",
        "        self.bitrate = bitrate\n",
        "\n",
        "        assert 0 <= amplitude <= 1, \"Amplitude is out of boundaries.\"\n",
        "        self.amplitude = amplitude\n",
        "\n",
        "        self.encoding = encoding\n",
        "\n",
        "\n",
        "    def sine_wave_generator(self, duration):\n",
        "\n",
        "        \"\"\"\n",
        "        Sinusoidal Wave Generating Function\n",
        "\n",
        "            duration : second(s)\n",
        "\n",
        "             returns : numpy.array() #sine wave\n",
        "        \"\"\"\n",
        "\n",
        "        # Generates from 0 to 1/Fbit with steps from fs\n",
        "\n",
        "        arranged = np.arange(0, duration, 1/self.samplingrate, dtype=np.float32)\n",
        "\n",
        "        # Carrier wave\n",
        "\n",
        "        sinusoidal_wave = np.sin(2 * np.pi * self.frequency * arranged)\n",
        "\n",
        "        return sinusoidal_wave\n",
        "\n",
        "    def smooth(self, data, n, curve=0.05):\n",
        "\n",
        "        \"\"\"\n",
        "        Frequency Change Damping Function\n",
        "\n",
        "        Smooths signal so the speakers won't break on frequency change\n",
        "\n",
        "           data : list\n",
        "              n : the factor of multiplication\n",
        "          curve : higher value results in smoother curve\n",
        "\n",
        "        returns : list #magnified ~n times\n",
        "        \"\"\"\n",
        "\n",
        "        # the function uses some werid parsing when multiplying the values,\n",
        "        # thus it has to be split as follows. This is a low priority bug.\n",
        "\n",
        "        data = [data[0], *data, data[-1]]\n",
        "\n",
        "        D = np.linspace(0,2, n)\n",
        "\n",
        "        sigmaD = 1 / (1 + np.exp(-(1 - D) / curve))\n",
        "\n",
        "        def sigma(x0, x1):\n",
        "\n",
        "            return x0 + (x1 - x0)*(1 - sigmaD)\n",
        "\n",
        "        result = [c for i in range(len(data)) if i+1 < len(data) for c in sigma(data[i],data[i+1])]\n",
        "\n",
        "        start, end = int(np.floor(n/2)), int(np.ceil(n/2))\n",
        "\n",
        "        return result[start:-end]\n",
        "\n",
        "\n",
        "    def modulate(self, payload):\n",
        "\n",
        "        \"\"\"\n",
        "        Amplitude Modulation Function\n",
        "\n",
        "           payload : list\n",
        "          curve : higher value results in smoother curve\n",
        "\n",
        "        returns : numpy.array() #modulated signal\n",
        "        \"\"\"\n",
        "\n",
        "        if type(payload).__name__ == \"str\":\n",
        "\n",
        "            payload = str_to_bin(payload, encoding=self.encoding)\n",
        "\n",
        "            payload = [int(i) for i in payload]\n",
        "\n",
        "\n",
        "        # data preprocessing\n",
        "\n",
        "        payload = [i if i==1 else self.amplitude for i in payload]\n",
        "\n",
        "        bit_length = int(self.samplingrate / self.bitrate)\n",
        "\n",
        "        pre_modulated_signal = np.array(self.smooth(payload, bit_length))\n",
        "\n",
        "        duration = len(pre_modulated_signal) / self.samplingrate\n",
        "\n",
        "        carrier = self.sine_wave_generator(duration)\n",
        "\n",
        "        carrier *= pre_modulated_signal\n",
        "\n",
        "        if max(abs(carrier)) > 1:\n",
        "\n",
        "          carrier /= max(abs(carrier))\n",
        "\n",
        "        return carrier\n",
        "\n",
        "# ================= End Modulation ======================="
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmKXfTTaQFt3"
      },
      "source": [
        "\n",
        "\n",
        "# dataset generation\n",
        "\n",
        "def add_white_noise(sig, k):\n",
        "\n",
        "    if not 0 <= k <= 1:\n",
        "        print(f\"K must be within 1 and 0 not {k}, k will now equal to 0.1\")\n",
        "        k = 0.1\n",
        "\n",
        "    n = len(sig)\n",
        "\n",
        "    white = np.array([np.random.random()*2-1 for i in range(n)]) * k\n",
        "\n",
        "    mixed = white + sig * (1-k)\n",
        "\n",
        "    if max(abs(mixed)) > 1:\n",
        "\n",
        "          mixed /= max(abs(mixed))\n",
        "\n",
        "    return mixed\n",
        "\n",
        "def generate_dataset(n, size):\n",
        "\n",
        "    kg = KernelGenerator()\n",
        "\n",
        "    kg.start()\n",
        "\n",
        "    bias = kg.fastgen([0.1, 0.4, 0.3, 0.2])\n",
        "\n",
        "    bias = kg.setwindow(0, 0.5, bias)\n",
        "\n",
        "    M = Modulation(frequency=8e3, bitrate=20)\n",
        "\n",
        "    bin = {1:0,0:1}\n",
        "\n",
        "\n",
        "\n",
        "    streams = []\n",
        "\n",
        "    names = []\n",
        "\n",
        "    final = []\n",
        "\n",
        "    for k in range(n):\n",
        "\n",
        "        data = []\n",
        "\n",
        "        for i in range(size):\n",
        "\n",
        "            if i not in [0,1]:\n",
        "                if data[-1] == data[-2]:\n",
        "                    data.append(bin[data[-2]])\n",
        "                    continue\n",
        "\n",
        "            data.append(np.random.choice([0,1]))\n",
        "\n",
        "        signal = M.modulate(data)\n",
        "\n",
        "        signal = add_white_noise(signal, np.random.choice(bias))\n",
        "\n",
        "        streams.append(signal)\n",
        "\n",
        "        names.append(np.array(data))\n",
        "\n",
        "        data.append(signal)\n",
        "\n",
        "        final.append(np.array(data))\n",
        "\n",
        "    return np.array(final)\n",
        "\n",
        "\n",
        "def generate(n):\n",
        "\n",
        "  # 0 = 0, 1 = 1, 00, = 2, 11 = 3\n",
        "\n",
        "  scheme = {\n",
        "                      '0':[0],\n",
        "                     '00':[0,0],\n",
        "                      '1':[1],\n",
        "                     '11':[1,1],\n",
        "  }\n",
        "\n",
        "  data_x = []\n",
        "\n",
        "  data_y = []\n",
        "\n",
        "  M = Modulation(frequency=2e3, bitrate=50)\n",
        "\n",
        "  kg = KernelGenerator()\n",
        "  kg.start()\n",
        "  bias = kg.fastgen([0.1, 0.4, 0.3, 0.2])\n",
        "  bias = kg.setwindow(0, 0.4, bias)\n",
        "\n",
        "  for i in range(n):\n",
        "\n",
        "    tag = np.random.choice(list(scheme.keys()))\n",
        "\n",
        "    data_y.append(tag)\n",
        "\n",
        "    signal = M.modulate(scheme[tag])\n",
        "\n",
        "    # print(np.zeros(np.random.randint(50, 1000)).tolist())\n",
        "\n",
        "    result = np.zeros(np.random.randint(50,100)).tolist()\n",
        "\n",
        "    result.extend(signal)\n",
        "\n",
        "    result.extend(np.zeros(np.random.randint(50, 100)).tolist())\n",
        "\n",
        "    signal = np.array(result)\n",
        "\n",
        "    # signal = add_white_noise(signal, np.random.choice(bias))\n",
        "\n",
        "    data_x.append(np.float32(signal))\n",
        "\n",
        "  return data_x, data_y\n",
        "    \n",
        "\n"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsD_KtNJOOzE"
      },
      "source": [
        "# data_x = [] # continious stream of modulated signals as either 0, 00, 1, 11\n",
        "\n",
        "# data_y = [] # The corresponding labels but will be modulated as 0 = 0, 1 = 1, 00, = 2, 11 = 3\n",
        "\n",
        "\n",
        "size = 1000\n",
        "data_x, data_y = generate(size)\n",
        "\n",
        "# preprocessing data\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "data_x = pad_sequences(data_x, maxlen=2100, dtype='float', padding='post', truncating='post', value=0.)\n",
        "\n",
        "data_x = data_x / np.max(data_x)\n",
        "\n",
        "data_x = data_x[:,:,np.newaxis]\n",
        "\n",
        "\n",
        "\n",
        "# Labeling\n",
        "# 0 = 0, 1 = 1, 00, = 2, 11 = 3\n",
        "\n",
        "data_y = pd.Series(data_y)\n",
        "data_y.value_counts()\n",
        "data_y = data_y.map({'0':0, '1':1, '00':2, '11':3}).values\n"
      ],
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RESZzZAqh2pQ",
        "outputId": "3fe1de5e-fd8b-495f-c77c-f6d050358060"
      },
      "source": [
        "# Creation of deep learning model\n",
        "\n",
        "from keras.layers import InputLayer, Conv1D, Dense, Flatten, MaxPool1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(InputLayer(input_shape=data_x.shape[1:]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.add(Conv1D(filters=500, kernel_size=10, activation='relu')) \n",
        "model.add(MaxPool1D(strides=16)) \n",
        "model.add(Conv1D(filters=50, kernel_size=10, activation='relu')) \n",
        "model.add(MaxPool1D(strides=8)) \n",
        "model.add(Flatten()) \n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# display model info\n",
        "model.summary()\n",
        "\n",
        "# model.add(Conv1D(filters=50, kernel_size=10, activation='relu')) \n",
        "# model.add(MaxPool1D(strides=8)) \n",
        "# model.add(Conv1D(filters=50, kernel_size=10, activation='relu')) \n",
        "# model.add(MaxPool1D(strides=8)) \n",
        "# model.add(Flatten()) \n",
        "# model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# # display model info\n",
        "# model.summary()"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_8 (Conv1D)            (None, 2091, 500)         5500      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1 (None, 131, 500)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_9 (Conv1D)            (None, 122, 50)           250050    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_9 (MaxPooling1 (None, 16, 50)            0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 4)                 3204      \n",
            "=================================================================\n",
            "Total params: 258,754\n",
            "Trainable params: 258,754\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "hqQQE_hwihj9",
        "outputId": "e0749b17-70fc-4a73-dbd9-e828dfd6b96c"
      },
      "source": [
        "# train model\n",
        "model.fit(data_x, data_y, batch_size=2000, epochs=10)\n",
        "\n",
        "# data_x.shape[1:]"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.8753 - accuracy: 0.2730\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.8804 - accuracy: 0.2830\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.8734 - accuracy: 0.2300\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.8724 - accuracy: 0.2410\n",
            "Epoch 5/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-258-5f9f08b4d2ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# data_x.shape[1:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7cdSiKioHbe"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import os\n",
        "#%% Specify parameters \n",
        "batch_size = 40                    #Note that large batch sized is linked to sharp gradients\n",
        "training_steps = 10                #Number of batches to train on\n",
        "num_epochs = 60                    #None to repeat dataset until all steps are executed\n",
        "eval_folder = 'G:\\powerLineData\\TFR_eval_sfft'     #Subfolder containing TFR files with evaluation data\n",
        "train_folder = 'G:\\powerLineData\\TFR_train_sfft'   #Subfolder containing TFR files with training data\n",
        "predict_folder = 'G:\\powerLineData\\TFR_predict_sfft'   #Subfolder containing TFR files with training data\n",
        "#%% Building the CNN Classifier\n",
        "def cnn_model_fn(features, labels, mode):\n",
        "  \"\"\"Model function for CNN.\"\"\"\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      pass\n",
        "  else:    \n",
        "      labels=tf.reshape(labels,[-1,1])\n",
        "  input_layer = tf.reshape(features[\"signal_data\"], [-1, 240, 200,1])\n",
        "  print(input_layer)\n",
        "  \n",
        "  # Convolutional Layer #1\n",
        "  conv1 = tf.layers.conv2d(\n",
        "      inputs=input_layer,\n",
        "      filters=32,\n",
        "      kernel_size=[5, 5],\n",
        "      strides=(2, 2),\n",
        "      padding=\"same\",\n",
        "      activation=tf.nn.relu)\n",
        "      #Output -1,120,100,32\n",
        "  print(conv1)\n",
        "\n",
        "  # Convolutional Layer #2 and Pooling Layer #2\n",
        "  conv2 = tf.layers.conv2d(\n",
        "      inputs=conv1,\n",
        "      filters=64,\n",
        "      kernel_size=[3, 3],\n",
        "      padding=\"same\",\n",
        "      activation=tf.nn.relu)\n",
        "  #Output -1,120,100,64\n",
        "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
        "  #Output -1,60,50,64\n",
        "  dropout = tf.layers.dropout(\n",
        "      inputs=pool2, rate=0.1, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "  \n",
        "  # Convolutional Layer #3 and Pooling Layer #3\n",
        "  conv3 = tf.layers.conv2d(\n",
        "      inputs=dropout,\n",
        "      filters=128,\n",
        "      kernel_size=[3, 3],\n",
        "      padding=\"same\",\n",
        "      activation=tf.nn.relu)\n",
        "  #Output -1,60,50,128\n",
        "  pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
        "  #Output -1,30,25,128\n",
        "  dropout2 = tf.layers.dropout(\n",
        "      inputs=pool3, rate=0.1, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "  \n",
        "  # Convolutional and pooling Layer #4\n",
        "  conv4 = tf.layers.conv2d(\n",
        "      inputs=dropout2,\n",
        "      filters=200,\n",
        "      kernel_size=[3, 3],\n",
        "      padding=\"same\",\n",
        "      activation=tf.nn.relu)\n",
        "  #Output -1,30,25,200 \n",
        "  pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=2)\n",
        "  #Output -1,15,12,200\n",
        "\n",
        "  # Dense Layer\n",
        "  pool4_flat = tf.reshape(pool4, [-1, 15 * 12 * 200])\n",
        "  dense = tf.layers.dense(inputs=pool4_flat, units=4096, activation=tf.nn.relu) \n",
        "  \n",
        "  dropout3 = tf.layers.dropout(\n",
        "      inputs=dense, rate=0.2, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "  \n",
        "  dense2 = tf.layers.dense(inputs=dropout3, units=2048, activation=tf.nn.relu)\n",
        "  \n",
        "  dropout4 = tf.layers.dropout(\n",
        "      inputs=dense2, rate=0.2, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "  # Logits Layer\n",
        "  logits = tf.layers.dense(inputs=dropout4, units=1)\n",
        "\n",
        "  predictions = {\n",
        "      # Generate predictions (for PREDICT and EVAL mode)\n",
        "      \"classes\": tf.round(tf.nn.sigmoid(logits)),\n",
        "      \"probabilities\": tf.nn.sigmoid(logits, name=\"probs_tensor\"),\n",
        "      \"signal_id\": tf.reshape(features[\"signal_ID\"],[-1,1])\n",
        "  }\n",
        "  \n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "  loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=labels, logits=logits)\n",
        "\n",
        "\n",
        "  # Configure the Training Op (for TRAIN mode)\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    \n",
        "    # Calculate Loss (for both TRAIN and EVAL modes) via cross entropy\n",
        "    \n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "    train_op = optimizer.minimize(\n",
        "        loss=loss,\n",
        "        global_step=tf.train.get_global_step())\n",
        "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
        "\n",
        "  # Add evaluation metrics (for EVAL mode)\n",
        "  eval_metric_ops = {\n",
        "          \"accuracy\": tf.metrics.auc(\n",
        "          labels=labels, predictions=predictions[\"classes\"])\n",
        "    \n",
        "  }\n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
        "\n",
        "#%% CREATE ESTIMATOR\n",
        "\n",
        "# Create the Estimator\n",
        "discharge_classifier = tf.estimator.Estimator(\n",
        "    model_fn=cnn_model_fn, model_dir=\"/tmp4096/sfft_convnet_model\")\n",
        "\n",
        "#%% Set Up a Logging Hook\n",
        "\n",
        "# Set up logging for predictions\n",
        "tensors_to_log = {\"probabilities\": \"probs_tensor\"}\n",
        "\n",
        "logging_hook = tf.train.LoggingTensorHook(\n",
        "    tensors=tensors_to_log, every_n_iter=50)\n",
        "\n",
        "#%% Input function for training data\n",
        "\n",
        "def dataset_input_fn(subfolder, batch_size, train = False, num_epochs=None):\n",
        "         \n",
        "    filenames = [file for file in os.listdir(subfolder) if file.endswith('.tfrecord')]\n",
        "    filenames = [os.path.join(subfolder, file) for file in filenames]\n",
        "    dataset = tf.data.TFRecordDataset(filenames)\n",
        "\n",
        "    #Create record extraction function\n",
        "    def parser(record):\n",
        "        features = {\n",
        "            'signal': tf.FixedLenFeature([50000], dtype=tf.float32),\n",
        "            'signal_ID': tf.FixedLenFeature([], dtype=tf.int64),\n",
        "            'measurement_ID': tf.FixedLenFeature([], dtype=tf.int64),\n",
        "            'label': tf.FixedLenFeature([], dtype=tf.int64)}\n",
        "        parsed = tf.parse_single_example(record, features)\n",
        "        \n",
        "        # Perform additional preprocessing on the parsed data.\n",
        "        bw_data = tf.reshape(tf.sqrt(parsed['signal']), [-1, 250, 200])\n",
        "        bw_data = tf.slice(bw_data, [0, 2, 0], [1, 240, 200])\n",
        "        \n",
        "        # Min max normalization\n",
        "        bw_data = tf.div(\n",
        "                tf.subtract(\n",
        "                    bw_data, \n",
        "                    tf.reduce_min(bw_data)\n",
        "                ), \n",
        "                tf.subtract(\n",
        "                    tf.reduce_max(bw_data), \n",
        "                    tf.reduce_min(bw_data)\n",
        "                )\n",
        "        )\n",
        "        bw_data = tf.round(bw_data)\n",
        "        \n",
        "        signal_data = tf.reshape(parsed['signal'], [-1, 250, 200])\n",
        "        #remove low frequency components\n",
        "        signal_data = tf.slice(signal_data, [0, 2, 0], [1, 240, 200])\n",
        "        \n",
        "        #Normalize and scale data\n",
        "        qube = tf.fill([240,200],1/3)\n",
        "        signal_data = tf.pow(signal_data,qube)\n",
        "        signal_data = tf.image.per_image_standardization(signal_data)\n",
        "        \n",
        "        norm_max = tf.fill([240,200],6.0)\n",
        "        signal_data = tf.divide(signal_data,norm_max)\n",
        "\n",
        "        label = tf.cast(parsed[\"label\"], tf.int32)\n",
        "    \n",
        "        return {\"signal_data\": signal_data, \"bw_data\": bw_data, \"signal_ID\": parsed[\"signal_ID\"]}, label\n",
        "\n",
        "    # Use `Dataset.map()` to build a pair of a feature dictionary and a label\n",
        "    # tensor for each example.\n",
        "    dataset = dataset.map(parser)\n",
        "    \n",
        "    #Shuffle data if in training mode\n",
        "    if train:\n",
        "        dataset = dataset.shuffle(buffer_size=batch_size*2)  #Shuffles along first dimension(rows)(!)  and selects from buffer\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.repeat(num_epochs)\n",
        "    \n",
        "    # Each element of `dataset` is tuple containing a dictionary of features\n",
        "    # (in which each value is a batch of values for that feature), and a batch of\n",
        "    # labels.\n",
        "    return dataset\n",
        "\n",
        "#%% Train the clasifier\n",
        "discharge_classifier.train(\n",
        "    input_fn=lambda : dataset_input_fn(train_folder, train = True, batch_size = batch_size, num_epochs=num_epochs),\n",
        "    steps=training_steps,\n",
        "    hooks=[logging_hook])\n",
        "\n",
        "#%% Evaluate the model\n",
        "eval_results = discharge_classifier.evaluate(\n",
        "        input_fn=lambda : dataset_input_fn(eval_folder, train = False, batch_size = batch_size, num_epochs=1))\n",
        "print(eval_results)\n",
        "\n",
        "#%% Predict\n",
        "results = discharge_classifier.predict(\n",
        "        input_fn=lambda : dataset_input_fn(predict_folder, train = False, batch_size = batch_size, num_epochs=1))\n",
        "results = list(results)\n",
        "\n",
        "#%% Get labels from TFR files\n",
        "with tf.Session() as sess:\n",
        "    dataset = dataset_input_fn(eval_folder, train = False, batch_size = 2178, num_epochs=2)\n",
        "    iterator = dataset.make_initializable_iterator()\n",
        "    sess.run(iterator.initializer)  \n",
        "    batch = iterator.get_next()\n",
        "    labels = batch[1].eval()\n",
        "    signal_ids = batch[0][\"signal_ID\"].eval()\n",
        "\n",
        "#%% MCC calculations\n",
        "#predicted_probs = np.array(list(map(lambda p: p['probabilities'],results)), dtype=np.float32)\n",
        "#predicted_class = np.array(list(map(lambda c: c['classes'],results)), dtype=np.int16)\n",
        "\n",
        "#Predict classes based on predicted probabilities and threshold\n",
        "def score_model_measurement(probs,threshold):\n",
        "    predicted = np.array([1 if x > threshold else 0 for x in probs[:,0]])           \n",
        "    return predicted\n",
        "\n",
        "#Print confusion matric and calculate Matthews correlation coefficient (MCC) \n",
        "def print_metrics(labels, scores):\n",
        "    conf = confusion_matrix(labels, scores)\n",
        "    print('                 Confusion matrix')\n",
        "    print('                 Score positive    Score negative')\n",
        "    print('Actual positive    %6d' % conf[1,1] + '             %5d' % conf[1,0])\n",
        "    print('Actual negative    %6d' % conf[0,1] + '             %5d' % conf[0,0])\n",
        "    print('')\n",
        "    print('Accuracy  %0.2f' % accuracy_score(labels, scores))\n",
        "    \n",
        "    TP = conf[1,1]\n",
        "    TN = conf[0,0]\n",
        "    FP = conf[0,1]\n",
        "    FN = conf[1,0]\n",
        "    MCC = ((TP*TN) - (FP*FN)) / np.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n",
        "    print('MCC = %0.2f' %MCC)\n",
        "    return MCC\n",
        "\n",
        "#Print confusion matrix and Matthews correlation coefficient (MCC) based on labels vs predictions\n",
        "#predictions = score_model_measurement(predicted_probs,0.5)\n",
        "#MCC = print_metrics(labels, predictions)\n",
        "\n",
        "#%% Training run with a custom validation each epoch\n",
        "\n",
        "loss_plot = np.array([])\n",
        "accuracy_plot = np.array([])\n",
        "MCC_plot = np.array([])\n",
        "epochs_plot = np.array([])\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    \n",
        "    discharge_classifier.train(\n",
        "    input_fn=lambda : dataset_input_fn(train_folder, train = True, batch_size = batch_size, num_epochs=1),\n",
        "    steps=None)\n",
        "    \n",
        "    eval_results = discharge_classifier.evaluate(\n",
        "        input_fn=lambda : dataset_input_fn(eval_folder, train = False, batch_size = batch_size, num_epochs=1))\n",
        "    \n",
        "    results = discharge_classifier.predict(\n",
        "        input_fn=lambda : dataset_input_fn(eval_folder, train = False, batch_size = batch_size, num_epochs=1))\n",
        "    predicted_probs = np.array(list(map(lambda p: p['probabilities'],results)), dtype=np.float32)\n",
        "    \n",
        "    scores = score_model_measurement(predicted_probs,0.5)\n",
        "    MCC = print_metrics(labels, scores)\n",
        "    \n",
        "    loss_plot = np.append(loss_plot,eval_results['loss'])\n",
        "    accuracy_plot = np.append(accuracy_plot,eval_results['accuracy'])\n",
        "    if np.isnan(MCC):\n",
        "        MCC=0\n",
        "    MCC_plot = np.append(MCC_plot,MCC)\n",
        "    epochs_plot = np.append(epochs_plot,i)\n",
        "    \n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(epochs_plot,loss_plot,color='lightcoral', marker='o', linestyle='--', linewidth=1.5, markersize=5, label='loss')\n",
        "    plt.plot(epochs_plot,accuracy_plot,color='steelblue', marker='s', linestyle='-.', linewidth=1.5, markersize=5,label='accuracy')\n",
        "    plt.plot(epochs_plot,MCC_plot,color='seagreen', marker='^', linestyle='-', linewidth=1.5, markersize=5,label='MCC')\n",
        "    \n",
        "    \n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.savefig(\"Learning_plot.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}